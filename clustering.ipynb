{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552a95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clustering_algorithms as ca\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import util\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c7e34c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>editor</th>\n",
       "      <th>title</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>pages</th>\n",
       "      <th>...</th>\n",
       "      <th>note</th>\n",
       "      <th>pdf</th>\n",
       "      <th>abstract</th>\n",
       "      <th>semantic_scholar</th>\n",
       "      <th>semantic_scholar_authorIds</th>\n",
       "      <th>semantic_scholar_keywords</th>\n",
       "      <th>cso_syntactic</th>\n",
       "      <th>cso_semantic</th>\n",
       "      <th>cso_union</th>\n",
       "      <th>cso_enhanced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.aclweb.org/anthology/2020.acl-main.1</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2020</td>\n",
       "      <td>July</td>\n",
       "      <td></td>\n",
       "      <td>Learning to Understand Child-directed and Adul...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>gelderloos-etal-2020-learning</td>\n",
       "      <td>1--6</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2020.acl-main.1.pdf</td>\n",
       "      <td>Speech directed to children differs from adult...</td>\n",
       "      <td>2020.acl-main.1.json</td>\n",
       "      <td>['7805500', '2756960', '103538973']</td>\n",
       "      <td>[1017215, 1588157]</td>\n",
       "      <td>[linguistics, acoustics, language acquisition,...</td>\n",
       "      <td>[speech signals, synthetic speech, linguistics...</td>\n",
       "      <td>[linguistics, automatic speech recognition, ac...</td>\n",
       "      <td>[speech recognition, signal processing, educat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.aclweb.org/anthology/2020.acl-main.2</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2020</td>\n",
       "      <td>July</td>\n",
       "      <td></td>\n",
       "      <td>Predicting Depression in Screening Interviews ...</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>rinaldi-etal-2020-predicting</td>\n",
       "      <td>7--18</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2020.acl-main.2.pdf</td>\n",
       "      <td>Despite the pervasiveness of clinical depressi...</td>\n",
       "      <td>2020.acl-main.2.json</td>\n",
       "      <td>['19320780', '2457504', '37202877']</td>\n",
       "      <td>[8505]</td>\n",
       "      <td>[linguistics, pattern languages, psycholinguis...</td>\n",
       "      <td>[latent variable, latent factor, linguistics, ...</td>\n",
       "      <td>[latent factor, linguistics, dialogue, pattern...</td>\n",
       "      <td>[matrix factorizations, argumentation, speech ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url  \\\n",
       "0  https://www.aclweb.org/anthology/2020.acl-main.1   \n",
       "1  https://www.aclweb.org/anthology/2020.acl-main.2   \n",
       "\n",
       "                                   publisher address  year month editor  \\\n",
       "0  Association for Computational Linguistics  Online  2020  July          \n",
       "1  Association for Computational Linguistics  Online  2020  July          \n",
       "\n",
       "                                               title      ENTRYTYPE  \\\n",
       "0  Learning to Understand Child-directed and Adul...  inproceedings   \n",
       "1  Predicting Depression in Screening Interviews ...  inproceedings   \n",
       "\n",
       "                              ID  pages  ... note                  pdf  \\\n",
       "0  gelderloos-etal-2020-learning   1--6  ...       2020.acl-main.1.pdf   \n",
       "1   rinaldi-etal-2020-predicting  7--18  ...       2020.acl-main.2.pdf   \n",
       "\n",
       "                                            abstract      semantic_scholar  \\\n",
       "0  Speech directed to children differs from adult...  2020.acl-main.1.json   \n",
       "1  Despite the pervasiveness of clinical depressi...  2020.acl-main.2.json   \n",
       "\n",
       "            semantic_scholar_authorIds semantic_scholar_keywords  \\\n",
       "0  ['7805500', '2756960', '103538973']        [1017215, 1588157]   \n",
       "1  ['19320780', '2457504', '37202877']                    [8505]   \n",
       "\n",
       "                                       cso_syntactic  \\\n",
       "0  [linguistics, acoustics, language acquisition,...   \n",
       "1  [linguistics, pattern languages, psycholinguis...   \n",
       "\n",
       "                                        cso_semantic  \\\n",
       "0  [speech signals, synthetic speech, linguistics...   \n",
       "1  [latent variable, latent factor, linguistics, ...   \n",
       "\n",
       "                                           cso_union  \\\n",
       "0  [linguistics, automatic speech recognition, ac...   \n",
       "1  [latent factor, linguistics, dialogue, pattern...   \n",
       "\n",
       "                                        cso_enhanced  \n",
       "0  [speech recognition, signal processing, educat...  \n",
       "1  [matrix factorizations, argumentation, speech ...  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load anthology with all information\n",
    "df = pd.read_csv(\"data/anthology_conferences.csv\", sep=\"|\", keep_default_na=False,\n",
    "                 converters={\"semantic_scholar_keywords\":lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                            \"cso_syntactic\":lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                            \"cso_semantic\":lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                            \"cso_union\":lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"),\n",
    "                            \"cso_enhanced\":lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \")})\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798e61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters the embeddings to these whose paper's year is <= last year\n",
    "def filter_embeddings(embeddings, df, last_year=None):\n",
    "    \n",
    "    relevant_embeddings = []\n",
    "    \n",
    "    if last_year == None:\n",
    "        relevant_embeddings = embeddings\n",
    "    else:\n",
    "        for i, row in df.iterrows():\n",
    "            if row[\"year\"] <= last_year:\n",
    "                relevant_embeddings.append(embeddings[i])\n",
    "                \n",
    "    return relevant_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09244e50",
   "metadata": {},
   "source": [
    "# Create clusters with the imported clustering algorithms\n",
    "- store them in a json file for further testing and evaluation\n",
    "- here we use only one embedding, i.e. paraphrase-mpnet-base-v2 with titles + abstracts (paraphrase-mpnet-base-v2_titles_abstracts.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b807a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, embeddings = util.load_embeddings(\"paraphrase-mpnet-base-v2_titles_abstracts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a4b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify clustering parameters\n",
    "pretrained_models = [\"paraphrase-mpnet-base-v2\"]\n",
    "text_sets = [\"title+abstract\"]\n",
    "cluster_algorithms = [\"kmeans\", \"agglomerative\", \"topic\"]\n",
    "num_clusters = {\"kmeans\":[10, 15, 20, 25, 30, 35, 40, 45, 50], \"agglomerative\": [None, 10, 15, 20, 25, 30, 35, 40, 45, 50], \"fast\": [None], \"topic\":[None]}\n",
    "distance_thresholds = {\"kmeans\":[None], \"agglomerative\": [None, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8], \"fast\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8], \"topic\": [None]}\n",
    "min_cluster_size = {\"kmeans\": [None], \"agglomerative\": [None], \"fast\": [None], \"topic\": [100, 150, 200, 250, 300, 350, 400, 500]}\n",
    "neighbors = {\"kmeans\": [None], \"agglomerative\": [None], \"fast\": [None], \"topic\": [2, 5, 10, 20, 50, 75, 100, 200, 400]}\n",
    "components = {\"kmeans\": [None], \"agglomerative\": [None], \"fast\": [None], \"topic\": [2, 5, 10, 15, 20, 30, 40, 60, 80, 100, int(len(embeddings[0])/4), int(len(embeddings[0])/2), len(embeddings[0])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b42ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the clustering algorithms based on the specified parameters\n",
    "def extensive_clustering(df, last_year=None):\n",
    "    \n",
    "    # For each algorithm/config pair store the clustering results\n",
    "    config2clusters = dict()\n",
    "    \n",
    "    # Use all pretrained models\n",
    "    for pretrained_model in pretrained_models:\n",
    "        config2clusters[pretrained_model] = dict()\n",
    "        \n",
    "        # combined with all text variants\n",
    "        for text_set in text_sets:\n",
    "            config2clusters[pretrained_model][text_set] = dict()\n",
    "\n",
    "            # Load embeddings corresponding to pretrained_model and text_set\n",
    "            if text_set == \"title\":\n",
    "                _, embeddings = util.load_embeddings(pretrained_model + \"_titles.pkl\")\n",
    "            elif text_set == \"title+abstract\":\n",
    "                _, embeddings = util.load_embeddings(pretrained_model + \"_titles_abstracts.pkl\")\n",
    "                \n",
    "            # Filter embeddings\n",
    "            embeddings = filter_embeddings(embeddings, df, last_year)\n",
    "            \n",
    "            # use all clustering algorithms\n",
    "            for algorithm in cluster_algorithms:\n",
    "                config2clusters[pretrained_model][text_set][algorithm] = dict()\n",
    "                alg_dict = config2clusters[pretrained_model][text_set][algorithm]\n",
    "\n",
    "                alg_dict[\"num_clusters\"] = dict()\n",
    "\n",
    "                # use all numbers of clusters\n",
    "                for num_cluster in num_clusters[algorithm]:\n",
    "                    alg_dict[\"num_clusters\"][num_cluster] = dict()\n",
    "\n",
    "                    alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"] = dict()\n",
    "                    \n",
    "                    # special case agglomerative clustering\n",
    "                    if algorithm == \"agglomerative\" and num_cluster != None:\n",
    "                        thresholds = [None]\n",
    "                    elif algorithm == \"agglomerative\" and num_cluster == None:\n",
    "                        thresholds = distance_thresholds[algorithm][1:]\n",
    "                    else:\n",
    "                        thresholds = distance_thresholds[algorithm]\n",
    "\n",
    "                    # use all distance thresholds\n",
    "                    for distance_threshold in thresholds:\n",
    "                        alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold] = dict()\n",
    "\n",
    "                        alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold][\"min_cluster_sizes\"] = dict()\n",
    "\n",
    "                        # use all cluster sizes\n",
    "                        for cluster_size in min_cluster_size[algorithm]:\n",
    "                            alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold][\"min_cluster_sizes\"][cluster_size] = dict()\n",
    "\n",
    "                            alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold][\"min_cluster_sizes\"][cluster_size][\"neighbors\"] = dict()\n",
    "\n",
    "                            # use all numbers of clusters\n",
    "                            for num_neighbors in neighbors[algorithm]:\n",
    "                                alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold][\"min_cluster_sizes\"][cluster_size][\"neighbors\"][num_neighbors] = dict()\n",
    "\n",
    "                                alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold][\"min_cluster_sizes\"][cluster_size][\"neighbors\"][num_neighbors][\"components\"] = dict()\n",
    "\n",
    "                                # use all number of components\n",
    "                                for component in components[algorithm]:\n",
    "                                    alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold][\"min_cluster_sizes\"][cluster_size][\"neighbors\"][num_neighbors][\"components\"][component] = dict()\n",
    "\n",
    "                                    d = alg_dict[\"num_clusters\"][num_cluster][\"distance_thresholds\"][distance_threshold][\"min_cluster_sizes\"][cluster_size][\"neighbors\"][num_neighbors][\"components\"][component]\n",
    "\n",
    "                                    # clustering\n",
    "                                    if algorithm == \"kmeans\":\n",
    "                                        cluster2indices, labels, centers = ca.kmeans(embeddings, num_clusters=num_cluster, random_state=42)\n",
    "                                    elif algorithm == \"agglomerative\":\n",
    "                                        cluster2indices, labels = ca.agglomerative_clustering(embeddings, num_clusters=num_cluster, distance_threshold=distance_threshold)\n",
    "                                    elif algorithm == \"fast\":\n",
    "                                        cluster2indices, labels, centers = ca.fast_clustering(embeddings, threshold=distance_threshold)\n",
    "                                    elif algorithm == \"topic\":\n",
    "                                        cluster2indices, labels = ca.topic_clustering(embeddings, n_neighbors=num_neighbors, n_components=component, min_cluster_size=cluster_size)\n",
    "\n",
    "                                    d[\"cluster2indices\"] = cluster2indices\n",
    "                                    d[\"labels\"] = labels\n",
    "\n",
    "                                    print(\"Finished alg={}, num_clusters={}, threshold={}, cluster_size={}, num_neighbors={}, n_components={}\".format(algorithm, num_cluster, distance_threshold, cluster_size, num_neighbors, component))\n",
    "    \n",
    "    return config2clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bb01d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dfd4329b2561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig2clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextensive_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/clusters/paraphrase-mpnet-base-v2_titles_abstracts_None.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig2clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfig2clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextensive_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-626a6dc2f3f7>\u001b[0m in \u001b[0;36mextensive_clustering\u001b[0;34m(df, last_year)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Use all pretrained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mconfig2clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_models' is not defined"
     ]
    }
   ],
   "source": [
    "config2clusters = extensive_clustering(df)\n",
    "with open(\"data/clusters/paraphrase-mpnet-base-v2_titles_abstracts_None.json\", \"w\") as jf:\n",
    "    json.dump(config2clusters, jf)\n",
    "    \n",
    "config2clusters = extensive_clustering(df, last_year=2019)\n",
    "with open(\"data/clusters/paraphrase-mpnet-base-v2_titles_abstracts_2019.json\", \"w\") as jf:\n",
    "    json.dump(config2clusters, jf)\n",
    "    \n",
    "config2clusters = extensive_clustering(df, last_year=2020)\n",
    "with open(\"data/clusters/paraphrase-mpnet-base-v2_titles_abstracts_2020.json\", \"w\") as jf:\n",
    "    json.dump(config2clusters, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923db1a9",
   "metadata": {},
   "source": [
    "# Create clusterings with prefiltered clusterings \n",
    "- store them in a json file for further testing and evaluation\n",
    "- here we use all embeddings and the configurations filtered in notebook cluster_evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081c2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the filtered clustering algorithms based on the filtered specified parameters\n",
    "def extensive_clustering(df, last_year=None):\n",
    "\n",
    "    # For each algorithm/config pair store the clustering results\n",
    "    config2clusters = dict()\n",
    "\n",
    "    # Load configs with best evaluation result on clustering results with embeddings\n",
    "    # from 'paraphrase-mpnet-base-v2' with titles and abstracts\n",
    "    with open(\"data/clusters/paraphrase-mpnet-base-v2_titles_abstracts_2019_best_configs.json\") as jf:\n",
    "        best_configs = json.load(jf)\n",
    "\n",
    "    # Use all available embeddings\n",
    "    for filename in os.listdir(\"data/embeddings/\"):\n",
    "        texts, embeddings = util.load_embeddings(filename)\n",
    "        \n",
    "        # Filter embeddings\n",
    "        embeddings = filter_embeddings(embeddings, df, last_year=last_year)\n",
    "        \n",
    "        # Extract pretrainde model and text set used for embedding creation\n",
    "        s = filename.split(\"_\")\n",
    "        pretrained_model = s[0]\n",
    "        if s[1] == \"titles\" and s[2] == \"abstracts\" and s[3] == \"sent\":\n",
    "            text_set = \"title+abstract\" + \"_sent_mean\"\n",
    "        elif s[1] == \"titles\" and s[2] == \"abstracts.pkl\":\n",
    "            text_set = \"title+abstract\"\n",
    "        elif s[1] == \"titles.pkl\":\n",
    "            text_set = \"title\"\n",
    "        else:\n",
    "            print(\"Warning\", s)\n",
    "\n",
    "        # Add pretrained model to dictionary\n",
    "        if pretrained_model not in config2clusters:\n",
    "            config2clusters[pretrained_model] = dict()\n",
    "        # Add text set to pretrained model in dictionary   \n",
    "        config2clusters[pretrained_model][text_set] = []\n",
    "        d = config2clusters[pretrained_model][text_set]\n",
    "\n",
    "        # Now use all configs in filtered configs\n",
    "        for config in best_configs:\n",
    "            algorithm = config[\"algorithm\"]\n",
    "            if config[\"num_clusters\"] == \"null\":\n",
    "                num_clusters = None\n",
    "            else:\n",
    "                num_clusters = int(config[\"num_clusters\"])\n",
    "            if config[\"distance_threshold\"] == \"null\":\n",
    "                threshold = None\n",
    "            else:\n",
    "                threshold = float(config[\"distance_threshold\"])\n",
    "            if config[\"min_cluster_size\"] == \"null\":\n",
    "                min_cluster_size = None\n",
    "            else:\n",
    "                min_cluster_size = int(config[\"min_cluster_size\"])\n",
    "            if config[\"neighbors\"] == \"null\":\n",
    "                neighbors = None\n",
    "            else:\n",
    "                neighbors = int(config[\"neighbors\"])\n",
    "            if config[\"components\"] == \"null\":\n",
    "                components = None\n",
    "            else:\n",
    "                components = int(config[\"components\"])\n",
    "\n",
    "            # Execute clustering\n",
    "            if algorithm == \"kmeans\":\n",
    "                cluster2indices, labels, centers = ca.kmeans(embeddings, num_clusters=num_clusters, random_state=42)\n",
    "            elif algorithm == \"agglomerative\":\n",
    "                cluster2indices, labels = ca.agglomerative_clustering(embeddings, num_clusters=num_clusters, distance_threshold=threshold)\n",
    "                centers = []\n",
    "            elif algorithm == \"fast\":\n",
    "                cluster2indices, labels, centers = ca.fast_clustering(embeddings, threshold=threshold)\n",
    "                centers = []\n",
    "            elif algorithm == \"topic\":\n",
    "                cluster2indices, labels = ca.topic_clustering(embeddings, n_neighbors=neighbors, n_components=components, min_cluster_size=min_cluster_size)\n",
    "                centers = []\n",
    "\n",
    "            c = config.copy()\n",
    "            \n",
    "            c.update({\"cluster2indices\": cluster2indices, \"labels\": labels, \"centers\": [list([float(y) for y in x]) for x in centers]})\n",
    "            d.append(c) \n",
    "            \n",
    "            print(\"Finished alg={}, num_clusters={}, threshold={}, cluster_size={}, num_neighbors={}, n_components={}\".format(algorithm, num_clusters, threshold, min_cluster_size, neighbors, components))\n",
    "                       \n",
    "    return config2clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096670a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished alg=kmeans, num_clusters=10, threshold=None, cluster_size=None, num_neighbors=None, n_components=None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_fast._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 2, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished alg=kmeans, num_clusters=15, threshold=None, cluster_size=None, num_neighbors=None, n_components=None\n",
      "Finished alg=kmeans, num_clusters=20, threshold=None, cluster_size=None, num_neighbors=None, n_components=None\n",
      "Finished alg=kmeans, num_clusters=25, threshold=None, cluster_size=None, num_neighbors=None, n_components=None\n"
     ]
    }
   ],
   "source": [
    "config2clusters = extensive_clustering(df, last_year=2019)\n",
    "with open(\"data/clusters/all_embeddings_all_text_sets_best_configs_2019.json\", \"w\") as jf:\n",
    "        json.dump(config2clusters, jf)\n",
    "\n",
    "config2clusters = extensive_clustering(df, last_year=2020)\n",
    "with open(\"data/clusters/all_embeddings_all_text_sets_best_configs_2020.json\", \"w\") as jf:\n",
    "        json.dump(config2clusters, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34f56a",
   "metadata": {},
   "source": [
    "# Perform selected final best clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45829451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings: paraphrase-distilroberta-base-v2_titles.pkl\n",
    "# algorithm: kmeans\n",
    "# #clusters: 20\n",
    "texts, embeddings = util.load_embeddings(\"paraphrase-distilroberta-base-v2_titles.pkl\")\n",
    "cluster2indices, labels, centers = ca.kmeans(embeddings, num_clusters=20, random_state=42)\n",
    "results = {\"cluster2indices\": cluster2indices, \"labels\": labels, \"centers\": [list([float(y) for y in x]) for x in centers]}\n",
    "with open(\"data/clusters/final_best_one_clustering.json\", \"w\") as jf:\n",
    "    json.dump(results, jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d76ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
